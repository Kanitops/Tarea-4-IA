{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Este cuaderno es una copia de trabajo para resolver la tarea sin modificar el original.\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yxbELmQZOmi"
   },
   "source": [
    "## Tarea 4 Reinforcement Learning - TICS315 - Inteligencia Artificial \n",
    "\n",
    "Integrantes del grupo:\n",
    "\n",
    "\n",
    "*   Integrante 1\n",
    "*   Integrante 2\n",
    "*   Integrante 3\n",
    "*   Integrante 4\n",
    "\n",
    "\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "El reinforcement learning es una t√©cnica en el aprendizaje de maquinas donde no damos feedback inmediatas a las m√°quinas, en este contexto comunmente referidas **agentes**, sino que dejamos se exploren y se desenvuelvan en el mundo. Luego, mientras se desenvuelven, damos recompensas si hacen una buena acci√≥n, y una penalizaci√≥n si hacen algo malo. Esto suena muy similar al aprendizaje supervisado que ya han visto con redes neuronales, pero hay una diferencia bastante sustancial.\n",
    "\n",
    "Las redes neuronales en aprendizaje supervisado reciben feedback, o una evaluaci√≥n de lo que han hecho, por **cada** acci√≥n que realizan. En un video juego por ejemplo, es dificil saber si dar un paso hacia adelante es algo positivo o negativo, por lo tanto es muy dificil saber si la acci√≥n es correcta o no; ese paso hacia adelante pudo haber influido en hacernos ganar, o perder el juego, imposible saber en ese momento.\n",
    "En cambio, con reinforcement learning damos feedback por acciones **buenas** y castigamos acciones **malas**, no nos preocupamos por acciones *intermedias* que no estamos seguros a donde nos pueden llevar, eso es parte de lo que el agente debe aprender. Es por esto que se llama *aprendizaje reforzado*, u otro nombre quiz√°s mas adecuado, aprendizaje con feedback retardado, porque le decimos despues de que hizo las acciones si estas est√°n bien o no.\n",
    "\n",
    "## Actividades\n",
    "\n",
    "En este *notebook* hay varias actividades, etiquetadas con **Pregunta**, que deber√°n realizar en base a la aplicaci√≥n de *Reinforcement Learning*. \n",
    "\n",
    "Primero introduciremos el ambiente en el que nos desenvolveremos. Existen librerias como *Gym* que presentan ambientes de videojuegos para probar y evaluar tecnicas de *reinforcemente learning*, pero son una caja negra para la mayoria de los casos y entrar al codigo es bastante complejo. Para mitigar esto, hemos dise√±ado nuestro propio ambiente, muy simple, que nos permitira jugar un peque√±o juego donde tenemos un **heroe** üôÉ que necesita recoger **trofeos** üèÜ y evitar a los **zombies** üßü."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPmfPbg8bVSE"
   },
   "source": [
    "## Ambiente para nuestro agente\n",
    "\n",
    "Aqu√≠ implementaremos las partes necesarias para que nuestro agente pueda vivir en este mundo.\n",
    "\n",
    "\n",
    "Primero necesitamos importar algunas cosas para facilitarnos la vida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 721,
     "status": "ok",
     "timestamp": 1725892522801,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "fJaoDoPXMzCQ"
   },
   "outputs": [],
   "source": [
    "# copy para copiar nuestros objetos de un lado para otro\n",
    "from copy import deepcopy, copy\n",
    "# numpy es una libreria numerica que permite facil trabajo con\n",
    "# matrices, como matlab. La usamos para trabajos numericos\n",
    "import numpy as np\n",
    "# para tener cosas random!\n",
    "import random\n",
    "# para ayudarnos con los tipos, nos ayuda a tener mas claro que retorna que\n",
    "from typing import List, Tuple, Any, Union, NewType, Dict\n",
    "\n",
    "# siempre es bueno usar una semilla cuando hacemos experimentos, es la unica\n",
    "# forma confiable que tenemos para asegurarnos que nuestros experimentos\n",
    "# son reproducibles\n",
    "random.seed(42)  # no importa que numero elijamos, pero lo dejamos fijo\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1725892523442,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "jFBSvEohM8FI"
   },
   "outputs": [],
   "source": [
    "# Vamos a definir algunas cosas con `monitos` para que nuestro mapa se\n",
    "# vea mas entretenido\n",
    "\n",
    "# Partiremos con estos\n",
    "ZOMBIE = \"üßü\"\n",
    "HERO = \"üôÉ\"\n",
    "TROPHIE = \"üèÜ\"\n",
    "EMPTY = \"‚ö™\"\n",
    "\n",
    "# despues ustedes tendran que agregar estos!\n",
    "BLOCK = \"üö´\"\n",
    "KEY = \"üîë\"\n",
    "DOOR = \"üö™\"\n",
    "SWORD = \"üó°Ô∏è\"\n",
    "\n",
    "\n",
    "# Nuestro agente tiene que saber las condiciones del mundo donde existe\n",
    "# en este mundo solo hay 4 acciones que puede hacer.\n",
    "# Moverse hacia `arriba`, `abajo`, `derecha` e `izquierda`, nada mas.\n",
    "# En algun otro ambiente, podriamos agregar mas acciones como saltar\n",
    "# atacar, comprar, etc. Pero mantendremos la simplicidad aqui.\n",
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "# Juntamos nuestras acciones para que queden ordenadas.\n",
    "ACTIONS = [UP, DOWN, LEFT, RIGHT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1725892523442,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "Xuyp-xM_leVN"
   },
   "outputs": [],
   "source": [
    "# Aqui vamos a crear nuestros tipos, esto nos ayudara a entender que hace\n",
    "# cada metodo y funcion que usemos mas claramente.\n",
    "Action = NewType('Action', int)\n",
    "GridElement = NewType('GridElement', str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GApT6pfUdZZD"
   },
   "source": [
    "Arriba hemos definimos nuestro mapa. Esto no es importante para entender el concepto de *reinforcement learning*, sino mas bien es una pura implementaci√≥n a mano de un mapa. Puedes saltarte toda la parte donde creamos la grilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1725892523442,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "j36sWY4KMzQy"
   },
   "outputs": [],
   "source": [
    "# Aqui definimos nuestra grilla que acturar√° como la base de nuestro mapa\n",
    "class Grid:\n",
    "    # Nuestro constructor toma una lista u otra grilla y la guarda\n",
    "    # se preocupa de copiarla para que no modifiquemos la anterior\n",
    "    def __init__(self, grid:Union['Grid', List[List[GridElement]]]=None) -> None:\n",
    "        assert grid is not None\n",
    "        if isinstance(grid, list):\n",
    "            self.grid = deepcopy(grid)\n",
    "        elif isinstance(grid, Grid):\n",
    "            self.grid = deepcopy(grid.grid)\n",
    "\n",
    "        # Guardamos el tama√±o de la grilla para trabajar mas rapido\n",
    "        self.x_lim = len(self.grid[0])\n",
    "        self.y_lim = len(self.grid)\n",
    "\n",
    "    # Nuestro metodo para comparar una grilla con otra\n",
    "    def __eq__(self, other:'Grid') -> bool:\n",
    "        return isinstance(other, Grid) and self.grid == other.grid\n",
    "\n",
    "    # Simpre es importante que si modificamos nuestra igualdad, tambien\n",
    "    # adaptemos nuestro hash\n",
    "    def __hash__(self) -> int:\n",
    "        return hash(str(self.grid))\n",
    "\n",
    "    # Cuando imprimimos una grilla, esta se mostrara como un\n",
    "    # mapa, como una matriz\n",
    "    def __str__(self) -> str:\n",
    "        return '\\n'.join([' '.join(str(e) for e in row) for row in self.grid])\n",
    "\n",
    "    # Este es un metodo muy util para indexar partes de la grilla\n",
    "    def __getitem__(self, position:Tuple[int, int]) -> GridElement:\n",
    "        assert type(position) == tuple\n",
    "        # necesitamos 2 coordenadas para saber que hay en esa posicion\n",
    "        assert len(position) == 2\n",
    "        x, y = position\n",
    "        # verificamos que las coordenadas esten dentro de la grilla\n",
    "        assert 0 < x <= self.x_lim\n",
    "        assert 0 < y <= self.y_lim\n",
    "        # retornamos el elemento que hay en esa posicion\n",
    "        return self.grid[self.y_lim-y][x-1]\n",
    "\n",
    "    # Este es un metodo muy util para insertar elementos en la grilla\n",
    "    def __setitem__(self, position:Tuple[int, int], value:GridElement) -> None:\n",
    "        assert type(position) == tuple\n",
    "        assert len(position) == 2\n",
    "        x, y = position\n",
    "        assert 0 < x <= self.x_lim\n",
    "        assert 0 < y <= self.y_lim\n",
    "        # igual que antes, pero ahora asignamos un elemento en vez de retornarlo\n",
    "        self.grid[self.y_lim-y][x-1] = value\n",
    "\n",
    "    # Una forma `fancy` de acceder a variables de una clase sin un `getter`\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int, int]:\n",
    "        return (self.x_lim, self.y_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Rx9v-DPixO-"
   },
   "source": [
    "Lo anterior pueden ignorarlo completamente, pero ahora si parte lo que nos importa!\n",
    "\n",
    "A continuaci√≥n definiremos una clase `State`, la cual representa el estado de nuestro agente en algun momento de su traves√≠a.\n",
    "Un estado tiene 3 cosas:\n",
    "\n",
    "\n",
    "1.   El mapa en ese momento. Es decir, tenemos una grilla dentro del estado. Esto representar√° el estado del mapa en cada instante.\n",
    "2.   Las posiciones de nuestro **heroe**, claramente necesitamos saber donde esta nuestro personaje en cada momento.\n",
    "3.   Los limites del mapa, para no salirnos del mapa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1725892523442,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "3L-t7bAufqw4"
   },
   "outputs": [],
   "source": [
    "# La clase `State` representara un estado del heroe\n",
    "class State:\n",
    "    # En nuestro constructor solo asignamos nuestras variables\n",
    "    def __init__(self, grid:Union[Grid, List[List[GridElement]]]=None,\n",
    "                 hero_pos:Tuple[int,int]=(1,1),\n",
    "                 collected_trophies:int=0,\n",
    "                 required_trophies:int=None,\n",
    "                 has_key:bool=False,\n",
    "                 has_sword:bool=False) -> None:\n",
    "\n",
    "        self.grid = Grid(grid=grid)\n",
    "        self.x_lim, self.y_lim = self.grid.shape\n",
    "        self.hero_x, self.hero_y = hero_pos\n",
    "        self.has_key = has_key\n",
    "        self.has_sword = has_sword\n",
    "        self.collected_trophies = collected_trophies\n",
    "        self.required_trophies = required_trophies\n",
    "        if self.required_trophies is None:\n",
    "            self.required_trophies = self._count_elements(TROPHIE)\n",
    "\n",
    "    # Misma forma `fancy` de acceder a la posicion del heroe\n",
    "    @property\n",
    "    def hero_pos(self) -> Tuple[int, int]:\n",
    "        return (self.hero_x, self.hero_y)\n",
    "\n",
    "    @property\n",
    "    def trophies_pending(self) -> int:\n",
    "        return max(self.required_trophies - self.collected_trophies, 0)\n",
    "\n",
    "    def _count_elements(self, element:GridElement) -> int:\n",
    "        return sum(row.count(element) for row in self.grid.grid)\n",
    "\n",
    "    # Para dibujar nuestro mapa con el heroe en la posicion actual\n",
    "    def __str__(self) -> str:\n",
    "        grid = deepcopy(self.grid)\n",
    "        grid[self.hero_x, self.hero_y] = HERO\n",
    "        inventory = []\n",
    "        if self.has_key:\n",
    "            inventory.append('üîë')\n",
    "        if self.has_sword:\n",
    "            inventory.append('üó°Ô∏è')\n",
    "        inv = ''.join(inventory) if inventory else 'vac√≠o'\n",
    "        status = (f\"Tesoros: {self.collected_trophies}/{self.required_trophies} \"\n",
    "                  f\"| Inventario: {inv}\")\n",
    "        return f\"{grid.__str__()}\\n{status}\"\n",
    "\n",
    "    # Un estado es igual a otro si las grillas y posiciones de los heroes son\n",
    "    # las mismas\n",
    "    def __eq__(self, other:'State') -> bool:\n",
    "        return isinstance(other, State) and             self.hero_pos == other.hero_pos and             self.grid == other.grid and             self.collected_trophies == other.collected_trophies and             self.required_trophies == other.required_trophies and             self.has_key == other.has_key and             self.has_sword == other.has_sword\n",
    "\n",
    "    # Igual que antes, por completitud debemos implementar cuando 2\n",
    "    # estados tienen el mismo hash\n",
    "    def __hash__(self) -> int:\n",
    "        return hash((str(self.grid), self.hero_pos,\n",
    "                    self.collected_trophies, self.required_trophies,\n",
    "                    self.has_key, self.has_sword))\n",
    "\n",
    "    # Este metodo nos ayuda a obtener que elemento se encuentra en una posicion\n",
    "    # determinada. Necesitamos el estado pasado para comparar ya que no tenemos\n",
    "    # historia, pero es una forma simple de implementar el mapa sin\n",
    "    # mucho codigo. Recuerda que estamos en una cadena de Markov, aqui no tenemos\n",
    "    # los estados pasados! Estos no afectan la decision que tomaremos ahora.\n",
    "    def get_element(self, position:Tuple[int,int], state:'State') -> GridElement:\n",
    "        assert type(position) == tuple\n",
    "        assert len(position) == 2\n",
    "        x, y = position\n",
    "        assert 0 < x <= self.x_lim\n",
    "        assert 0 < y <= self.y_lim\n",
    "\n",
    "        # Por limitaciones de la implementacion, debemos saber si el heroe se\n",
    "        # movio a la posicion en la que esta, o estaba ahi desde antes\n",
    "        # otra implementacion podria solucionar este problema de mejor manera\n",
    "        # pero es mas compleja de entender\n",
    "        if position == state.hero_pos:\n",
    "            return HERO\n",
    "        return self.grid[x,y]\n",
    "\n",
    "    # De nuestras acciones tenemos que elegir una y actuar acorde a ella.\n",
    "    # por ejemplo, si le pedimos al estado que suba, entonces tenemos que\n",
    "    # enviar la accion `UP`.\n",
    "    # Cuando llamamos a este metodo, creamos un nuevo estado con la\n",
    "    # accion aplicada\n",
    "    def action_dispatch(self, action:Action) -> 'State':\n",
    "        if action == UP:\n",
    "            return self.moveUp()\n",
    "        elif action == DOWN:\n",
    "            return self.moveDown()\n",
    "        elif action == LEFT:\n",
    "            return self.moveLeft()\n",
    "        elif action == RIGHT:\n",
    "            return self.moveRight()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown action {action}\")\n",
    "\n",
    "    # Este metodo solo copia el estado actual y crea uno nuevo para aplicar\n",
    "    # los cambios pedidos por la accion ingresada\n",
    "    def register(self) -> 'State':\n",
    "        past_state = copy(self)\n",
    "        return State(grid=past_state.grid,\n",
    "                     hero_pos=past_state.hero_pos,\n",
    "                     collected_trophies=past_state.collected_trophies,\n",
    "                     required_trophies=past_state.required_trophies,\n",
    "                     has_key=past_state.has_key,\n",
    "                     has_sword=past_state.has_sword)\n",
    "\n",
    "    # Los siguientes metodos mueven nuestro personaje en las direcciones\n",
    "    # que definimos antes, arriba, abajo, derecha e izquierda\n",
    "\n",
    "    def moveUp(self) -> 'State':\n",
    "        new_state = self.register()\n",
    "        new_state.hero_y = new_state.hero_y + 1 if new_state.hero_y < new_state.y_lim else new_state.hero_y\n",
    "\n",
    "        return new_state\n",
    "\n",
    "    def moveDown(self) -> 'State':\n",
    "        new_state = self.register()\n",
    "        new_state.hero_y = new_state.hero_y - 1 if new_state.hero_y > 1 else new_state.hero_y\n",
    "\n",
    "        return new_state\n",
    "\n",
    "    def moveRight(self) -> 'State':\n",
    "        new_state = self.register()\n",
    "        new_state.hero_x = new_state.hero_x + 1 if new_state.hero_x < new_state.x_lim else new_state.hero_x\n",
    "\n",
    "        return new_state\n",
    "\n",
    "    def moveLeft(self) -> 'State':\n",
    "        new_state = self.register()\n",
    "        new_state.hero_x = new_state.hero_x - 1 if new_state.hero_x > 1 else new_state.hero_x\n",
    "\n",
    "        return new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g70vnhBcm1Bd"
   },
   "source": [
    "Listo, con esto hemos definido las bases para que nuestro **agente** pueda moverse libremente por el mundo que creemos.\n",
    "Probemos a ver como funciona!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1725892523442,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "cbMhqHTpqDEs",
    "outputId": "8934fb86-5e32-406c-edd7-6664d07ec897"
   },
   "outputs": [],
   "source": [
    "# Creamos una lista de listas (una matriz) que represente a nuestro mapa\n",
    "mapa_ejemplo = [\n",
    "    [TROPHIE, EMPTY, EMPTY],\n",
    "    [ZOMBIE, EMPTY, ZOMBIE],\n",
    "    [EMPTY, EMPTY, EMPTY]\n",
    "]\n",
    "# Digamos que nuestro heroe parte en la posicion (1,1)\n",
    "estado_ejemplo = State(grid=mapa_ejemplo, hero_pos=(1, 1))\n",
    "\n",
    "# Veamos como se ve!\n",
    "print(estado_ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw4LeuzVqCBE"
   },
   "source": [
    "Ahi esta nuestro **h√©roe** üôÉ! Tambi√©n podemos ver nuestro trofeo üèÜ a cual queremos llegar, y los zombies üßü que debemos evitar en el camino. Vamos a representar el camino libre como un circulo blanco ‚ö™.\n",
    "\n",
    "Ahora empieza la parte de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9xgY1HWnBRJ"
   },
   "source": [
    "## Feedback para el **agente**\n",
    "\n",
    "Hemos creado nuestro estado, nuestro mapa y todo, pero ahora necesitamos de alguna forma ver que acciones merecen un premio para el **agente** y cuales un castigo.\n",
    "\n",
    "- [x] Mapa\n",
    "- [x] Definici√≥n de un estado\n",
    "- [ ] Cu√°ndo dar recompensas y cu√°ndo castigar\n",
    "- [ ] Aprender...\n",
    "\n",
    "Esto lo definiremos en una funci√≥n que llamaremos `act`. La funci√≥n `act` necesita un `State` `s` y un `Action` `a` como argumentos, para simular un movimiento, desde un estado `s` mediante la acci√≥n `a`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVZoxljBvHrE"
   },
   "source": [
    "## Listos para aprender!\n",
    "\n",
    "Ahora tenemos todo nuestro ambiente implementado y definido.\n",
    "\n",
    "- [x] Mapa\n",
    "- [x] Definici√≥n de un estado\n",
    "- [x] Cu√°ndo dar recompensas y cuando castigar\n",
    "- [ ] Aprender...\n",
    "\n",
    "Lo que necesitamos ahora es algun algoritmo que nos ayude a aprender que acciones son buenas y cuales no. Para esto usaremos un algoritmo llamado *Q-Learning*, que es lo que vimos en la clase te√≥rica antes.\n",
    "\n",
    "Para esto, primero necesitamos una tabla donde iremos guardando cada uno de los estados y sus puntajes para cada acci√≥n. Es decir, dado un estado `s`, que deber√≠amos hacer ahora. Esta decisi√≥n se toma de acuerdo a un puntaje que va asociado a cada acci√≥n dado cada estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1725892523442,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "LPzvQBy1wuQs"
   },
   "outputs": [],
   "source": [
    "# Declaramos nuestra tabla `q_table` como un diccionario vacio\n",
    "# Nuestra tabla se vera de la siguiente forma:\n",
    "# estado: [lista de acciones posibles]\n",
    "# Esta lista de acciones posibles es una lista de puntajes para cada\n",
    "# accion dado un estado.\n",
    "q_table = {}\n",
    "\n",
    "# Luego hacemos nuestra funcion de busqueda `q`.\n",
    "# Esta tiene 2 funciones:\n",
    "# 1. Dado  un estado, retorna una lista con los puntajes para cada accion en ese\n",
    "# estado. Es decir, una lista con puntajes para decidir que hacer\n",
    "# 2. Dado un estado y una accion, retorna el puntaje asociado a realizar\n",
    "# esa accion en ese estado.\n",
    "def q(state:State, action:Action=None) -> Union[float, np.ndarray]:\n",
    "    if state not in q_table:\n",
    "        # Si no hemos visto este estado, lo creamos\n",
    "        # como no sabemos que hacer aun, decimos que todas las acciones\n",
    "        # tienen beneficio 0, ya que no lo hemos evaluado aun\n",
    "        q_table[state] = np.zeros(len(ACTIONS))\n",
    "\n",
    "    if action is None:\n",
    "        return q_table[state]\n",
    "\n",
    "    return q_table[state][action]\n",
    "\n",
    "# Este es un metodo conveniente para no estar borrando manualmente\n",
    "# la tabla cada vez que queremos hacer algo nuevo\n",
    "def reset_table():\n",
    "    global q_table\n",
    "    q_table = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1725892523442,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "nPi4H0QPMzTx"
   },
   "outputs": [],
   "source": [
    "# Definimos una funcion que represente un `acto`. Es decir\n",
    "# dado un estado y una accion, que ocurre.\n",
    "# Este \"que ocurre\" es bastante variado, podemos movernos,\n",
    "# ganar puntaje, perder el juego, o cualquier cosa que decidamos\n",
    "\n",
    "# Aqui es donde debemos decidir cuando y cuanta recompensa o castigo\n",
    "# debemos dar a nuestro agente.\n",
    "# Esta funcion retornara 3 cosas. El nuevo estado en que quedo nuestro heroe,\n",
    "# una recompensa por su esfuerzo (puede ser negativa), y un booleano indicando\n",
    "# si el juego termino o no. Este `termino` puede ser porque ganamos o perdimos.\n",
    "def act(state:State, action:Action):\n",
    "\n",
    "    # Le decimos a nuestro estado que se mueva en la direccion pedida\n",
    "    # esto nos da un nuevo estado\n",
    "    new_state = state.action_dispatch(action)\n",
    "\n",
    "    # ahora le pedimos al nuevo estado que nos diga que hay\n",
    "    # en la posicion que quedamos\n",
    "    # De nuevo, por un tema de implementacion tenemos que saber si donde estamos\n",
    "    # ahora estaba ocupado por otro elemento, o si siempre estuvimos nosotros ahi.\n",
    "    grid_item = new_state.get_element(new_state.hero_pos, state)\n",
    "\n",
    "    # Recompensas por defecto\n",
    "    reward = -1\n",
    "    is_done = False\n",
    "\n",
    "    if grid_item == ZOMBIE:\n",
    "\n",
    "        if new_state.has_sword:\n",
    "            # Podemos derrotarlo y seguir avanzando\n",
    "            new_state.grid[new_state.hero_pos] = EMPTY\n",
    "            reward = 200\n",
    "            is_done = False\n",
    "        else:\n",
    "            reward = -100\n",
    "            is_done = True\n",
    "\n",
    "    elif grid_item == TROPHIE:\n",
    "        new_state.grid[new_state.hero_pos] = EMPTY\n",
    "        new_state.collected_trophies += 1\n",
    "        if new_state.collected_trophies >= new_state.required_trophies:\n",
    "            reward = 1000\n",
    "            is_done = True\n",
    "        else:\n",
    "            reward = 250\n",
    "            is_done = False\n",
    "\n",
    "    elif grid_item == EMPTY:\n",
    "        reward = -1\n",
    "        is_done = False\n",
    "\n",
    "    elif grid_item == HERO:\n",
    "        reward = -1\n",
    "        is_done = False\n",
    "\n",
    "    elif grid_item == BLOCK:\n",
    "        reward = -15\n",
    "        return state, reward, False\n",
    "\n",
    "    elif grid_item == DOOR:\n",
    "        if new_state.has_key:\n",
    "            new_state.grid[new_state.hero_pos] = EMPTY\n",
    "            reward = -1\n",
    "            is_done = False\n",
    "        else:\n",
    "            reward = -25\n",
    "            return state, reward, False\n",
    "\n",
    "    elif grid_item == KEY:\n",
    "        new_state.has_key = True\n",
    "        new_state.grid[new_state.hero_pos] = EMPTY\n",
    "        reward = 75\n",
    "        is_done = False\n",
    "\n",
    "    elif grid_item == SWORD:\n",
    "        new_state.has_sword = True\n",
    "        new_state.grid[new_state.hero_pos] = EMPTY\n",
    "        reward = 75\n",
    "        is_done = False\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown grid item {grid_item}\")\n",
    "\n",
    "    return new_state, reward, is_done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q9_KqVHNFFO"
   },
   "source": [
    "Para terminar nuestras configuraciones, es necesario que digamos cuantas veces intentaremos correr el juego, y tambien la duraci√≥n del juego. Por ejemplo, podr√≠amos estar jugando 1 hora por 2 semanas, o 14 horas en un puro dia. Para asimilar lo aprendido, primero debemos dormir, es decir, dejar de jugar.\n",
    "\n",
    "Este ejemplo de dormir y tiempo entre juegos es una analogia muy util para describir el concepto de `episodios` y `pasos` de nuestro **agente**. Por cada `episodio` nuestro agente comienza el juego denuevo y podemos darnos cuenta si mejor√≥ o no, entonces no nos sirve solo hacer un `episodio` super largo si no tendremos la oportunidad de verificar los resultados; pero si tenemos demasiados episodios, no terminaremos de jugar nunca. El \"largo\" del juego viene dado por los `pasos`. Si son muy pocos `pasos` puede que no alcancemos a aprender lo que queremos, pero si son muchos puede que estemos perdiendo el tiempo y ya hayamos encontrado lo que buscabamos.\n",
    "\n",
    "A continuacion definiremos estas constantes para nuestro problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1725892523442,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "Ot-A9d6kMzYK"
   },
   "outputs": [],
   "source": [
    "# El total de episodios donde nuestro agente aprendera\n",
    "N_EPISODES = 400\n",
    "\n",
    "# El maximo numero de pasos por episodio\n",
    "MAX_EPISODE_STEPS = 200\n",
    "\n",
    "# Debemos definir nuestro conjunto de pesos de entrenamiento.\n",
    "# En un comienzo nuestro agente aprendera mucho, ya que sus primeros\n",
    "# acercamientos al juego son mas valiosos. Pero mientras mas veces jugamos\n",
    "# lo que aprendemos por cada jugada es cada vez menos. Es importante hacer esta\n",
    "# diferencia o una jugada muy avanzada, por intentar explorar, podria\n",
    "# arruinar todo lo que habiamos aprendido antes.\n",
    "\n",
    "# Siempre aprenderemos aun que sea un poco\n",
    "MIN_ALPHA = 0.05\n",
    "# Aprenderemos desde TODO, hasta un 5% de lo que veamos.\n",
    "alphas = np.linspace(1.0, MIN_ALPHA, N_EPISODES)\n",
    "\n",
    "# Un factor de descuento. Esto lo usamos para balancear entre la recompensa\n",
    "# maxima a corto plazo, o a largo plazo. Si lo dejamos solo a corto plazo\n",
    "# es poco probable que aprendamos algo util a futuro. Pero si lo dejamos en\n",
    "# 100% entonces estamos pensando demasiado en el futuro y no nos estamos\n",
    "# preocupando del presente.\n",
    "gamma = 0.95\n",
    "\n",
    "# Valores de epsilon para cada episodio, asi balanceamos exploracion\n",
    "EPSILON_START = 0.4\n",
    "EPSILON_END = 0.05\n",
    "epsilons = np.linspace(EPSILON_START, EPSILON_END, N_EPISODES)\n",
    "\n",
    "# Aqui simulamos la eleccion de una accion. Dado un estado, nos dice que\n",
    "# accion tomar. Existe un epsilon de probabilidad de que tomemos una accion\n",
    "# al azar.\n",
    "def choose_action(state:State, epsilon:float) -> Action:\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        return np.argmax(q(state))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o83QfpM9S6Cr"
   },
   "source": [
    "Listo! Implementemos `Q-learning` para que nuestro heroe üôÉ aprenda como obtener los trofeos üèÜ!\n",
    "\n",
    "Seg√∫n lo visto en clases la siguiente ecuaci√≥n se basa en las *cadenas de Markov*. La formula para aprender mediante `Q-learning` viene dada por la siguiente expresin:\n",
    "\n",
    "$$ Q^{nuevo}(s_t, a_t) =\n",
    "    \\underbrace{Q(s_t, a_t)}_{\\text{valor antiguo}} +\n",
    "    \\underbrace{\\alpha}_{\\text{la tasa de aprendizaje}} *\n",
    "        \\overbrace{\n",
    "            \\left(\\underbrace{r_t}_{\\text{recompensa}} +\n",
    "            \\underbrace{\\gamma}_{\\text{factor descuento}} *\n",
    "                \\underbrace{\\max_{acciones}(Q(s_{t+1},acciones))}_{\\text{valor optimo futuro}} -\n",
    "                \\underbrace{Q(s_t, a_t)}_{\\text{diferencia temporal}}\n",
    "            \\right)}\n",
    "        ^{\\text{lo que aprendimos}}\n",
    "$$\n",
    "\n",
    "B√°sicamente, para el estado actual, debemos ajustar el valor que nos dice que accion tomar basandonos en cual creemos que es el estado que nos da una mayor recompensa en el futuro. Es decir, desde el estado donde estamos, que acci√≥n nos lleva a un estado de mayor recompensa. El t√©rmino $Q(s_t, a_t)$ es importante porque representa la diferencia temporal del estado actual con el siguiente. No hay garant√≠a que visitar 2 veces el mismo estado nos de la misma recompensa, por lo que hay que considerar que existe el tiempo en nuestra ecuacion.\n",
    "\n",
    "En el siguiente trozo de c√≥digo implementamos esta ecuaci√≥n en un loop `for`. Para cada n√∫mero de `episodios`, avanzamos/ejecutamos hasta que el juego termina (porque ganamos o perdimos) o hasta que alcancemos el m√°ximo de `pasos`. Es importante se√±alar que no todos los juegos *terminan*, asi que es importante que tengamos un l√≠mite hasta cuando queremos seguir. Tambi√©n, si nos quedamos parados en el mismo lugar sin movernos, y no hay condiciones de tiempo, el juego durar√° para siempre. Hay que arreglar esos detalles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1725892523442,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "8ITT2755S6LH"
   },
   "outputs": [],
   "source": [
    "def q_learning(start_state:State, episodes:int, steps:int,\n",
    "               table:Dict[State, np.ndarray], learning_rate:np.ndarray,\n",
    "               discount:float, exploration_schedule:np.ndarray) -> List[float]:\n",
    "\n",
    "    episode_rewards = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "\n",
    "        # Creamos una copia para no modificar nuestro estado original\n",
    "        state = deepcopy(start_state)\n",
    "\n",
    "        # Partimos con una recompensa 0\n",
    "        total_reward = 0\n",
    "\n",
    "        # Cada episodio tiene una tasa de aprendizaje distinto\n",
    "        alpha = learning_rate[ep]\n",
    "        epsilon = exploration_schedule[ep]\n",
    "\n",
    "        # Para cada paso, vamos a ir actualizando nuestra tabla\n",
    "        # para encontrar los movimientos que nos llevaran a ganar el juego\n",
    "        for _ in range(steps):\n",
    "\n",
    "            # Tomamos una accion de nuestro banco de acciones\n",
    "            # dado nuestro estado\n",
    "            action = choose_action(state, epsilon)\n",
    "\n",
    "            # Llamamos un `acto`, para ver si lo hicimos bien\n",
    "            # necesitamos indicarle el estado donde estamos y la accion a\n",
    "            # realizar\n",
    "            # Esto nos da un nuevo estado, una recompensa y nos dice\n",
    "            # si se termino el juego o no.\n",
    "            next_state, reward, done = act(state, action)\n",
    "\n",
    "            # Vamos guardando nuestras recompensas\n",
    "            total_reward += reward\n",
    "\n",
    "            # Actualizamos nuestros estados con la formula que vimos antes\n",
    "            q(state)[action] = q(state, action) +                 alpha * (reward + discount * np.max(q(next_state)) - q(state, action))\n",
    "\n",
    "            # estamos listos para el siguiente paso\n",
    "            state = next_state\n",
    "\n",
    "            # si el juego termino, dejamos los pasos y comenzamos con un\n",
    "            # nuevo episodio\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        print(f\"Episode {ep + 1}: total reward -> {total_reward}\")\n",
    "\n",
    "    return episode_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qshfYj-VqKJm"
   },
   "source": [
    "## Listos para jugar!\n",
    "\n",
    "Ahora que ya tenemos nuestro estado, nuestro mapa, sabemos cuando darle recompensas a nuestro **agente**, y ademas implementamos la f√≥rmula de arriba para aprender, solo nos queda ejecutar nuestro programa y ver como nuestro h√©roe üôÉ esquiva los zombies üßü para obtener el trofeo üèÜ!\n",
    "\n",
    "- [x] Mapa\n",
    "- [x] Definici√≥n de un estado\n",
    "- [x] Cuando dar recompensas y cuando castigar\n",
    "- [x] Aprender...\n",
    "\n",
    "Creemos nuestro mapa :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1725892523442,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "Ca_6LCApMzbJ",
    "outputId": "c1bc2d94-13bd-42fe-e8a9-b62e236bfbd6"
   },
   "outputs": [],
   "source": [
    "# Dibujamos nuestro mapa. Pueden dibujar lo que quieran, a ver si el\n",
    "# agente se la puede\n",
    "grid = [\n",
    "    [TROPHIE, EMPTY, EMPTY, EMPTY, ZOMBIE, TROPHIE, BLOCK, BLOCK],\n",
    "    [ZOMBIE, ZOMBIE, BLOCK, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n",
    "    [DOOR, EMPTY, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n",
    "    [TROPHIE, BLOCK, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n",
    "    [ZOMBIE, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, BLOCK],\n",
    "    [EMPTY, EMPTY, BLOCK, EMPTY, BLOCK, BLOCK, EMPTY, BLOCK],\n",
    "    [EMPTY, ZOMBIE, BLOCK, KEY, BLOCK, BLOCK, EMPTY, BLOCK],\n",
    "    [EMPTY, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, SWORD, BLOCK]\n",
    "]\n",
    "\n",
    "\n",
    "# Y aqui definimos nuestro estado inicial. Pueden poner al heroe donde\n",
    "# quieran, pero cuiden que no parte sobre un zombie!\n",
    "initial_state = State(grid=grid, hero_pos=(1, 1))\n",
    "\n",
    "print(initial_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTbdoDpls8xS"
   },
   "source": [
    "## Ejecutando las guias de aprendizaje\n",
    "\n",
    "A continuaci√≥n vamos a llamar a la funci√≥n que har√° que nuestra `q_table` se llene de informaci√≥n util y representativa del juego que queremos ganar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1725892524102,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "0RVj4G6bwDWx",
    "outputId": "aa90b5c9-b506-4d3b-f4d2-8cd9df55a829"
   },
   "outputs": [],
   "source": [
    "# por si acaso, antes de entrenar reiniciamos nuestra tabla para no tener\n",
    "# informacion demas\n",
    "reset_table()\n",
    "\n",
    "# Le pasamos los argumentos a nuestra funcion de aprendizaje y estamos\n",
    "# listos para ver los resultados de nuestro agente\n",
    "training_rewards = q_learning(start_state=initial_state,\n",
    "           episodes=N_EPISODES,\n",
    "           steps=MAX_EPISODE_STEPS,\n",
    "           table=q_table,\n",
    "           learning_rate=alphas,\n",
    "           discount=gamma,\n",
    "           exploration_schedule=epsilons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_H0UslQxm3b"
   },
   "source": [
    "## Viendo los resultados\n",
    "\n",
    "Bueno, la funci√≥n de antes me mostr√≥ algunos n√∫meros pero s√© si en verdad el h√©roe üôÉ habr√° logrado su comentido?\n",
    "Ejecuta el c√≥digo de abajo para que se muestre lo aprendido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1725892524102,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "dgAnCFsOxl3V",
    "outputId": "7d969365-e9cd-4c91-ae3d-6d2f4a3ebe4c"
   },
   "outputs": [],
   "source": [
    "# Solo en caso de que nuestro heroe se quede pegado, ponemos un maximo\n",
    "# de escenarios a mostrar, aqui tenemos maximo 100\n",
    "show_max = 100\n",
    "\n",
    "# Partimos con el estado inicial, preguntemos a donde podemos movernos\n",
    "possible_actions = q(initial_state)\n",
    "print(initial_state)\n",
    "print(f\"up={possible_actions[UP]}, \"\n",
    "      f\"down={possible_actions[DOWN]}, \"\n",
    "      f\"left={possible_actions[LEFT]}, \"\n",
    "      f\"right={possible_actions[RIGHT]}\")\n",
    "\n",
    "print(f\"Inventario inicial -> llave={initial_state.has_key}, espada={initial_state.has_sword}\")\n",
    "print(f\"Tesoros pendientes -> {initial_state.trophies_pending}\")\n",
    "\n",
    "# Seleccionamos la accion con el mejor puntaje, y le pedimos al\n",
    "# heroe que se mueva en esa direccion. Esto nos da un nuevo estado\n",
    "s, _, done = act(initial_state, np.argmax(possible_actions))\n",
    "\n",
    "# Mientras no haya terminado el juego, o nos hayamos pasado del maximo de\n",
    "# acciones a mostar definido mas arriba, seguimos moviendonos con la\n",
    "# accion mas favorable para ese estado\n",
    "while not done and show_max:\n",
    "    # Mostramos el estado actual\n",
    "    print(s)\n",
    "    # vemos nuestras acciones\n",
    "    possible_actions = q(s)\n",
    "    # Que accion deberiamos tomar?\n",
    "    print(f\"up={possible_actions[UP]}, \"\n",
    "      f\"down={possible_actions[DOWN]}, \"\n",
    "      f\"left={possible_actions[LEFT]}, \"\n",
    "      f\"right={possible_actions[RIGHT]}\")\n",
    "    print(f\"Inventario -> llave={s.has_key}, espada={s.has_sword} | Tesoros pendientes={s.trophies_pending}\")\n",
    "    # Elejimos la mejor accion y continuamos\n",
    "    s, _, done = act(s, np.argmax(possible_actions))\n",
    "\n",
    "    show_max -= 1\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnzUuNcQ1B2u"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Preguntas\n",
    "\n",
    "Esperamos haya sido entretenido ver como nuestro h√©roe üôÉ lograba esquivar los zombies üßü para llegar al trofeo üèÜ. Ahora les toca a ustedes mejorar lo que les mostramos y responder las siguientes preguntas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTm4i4km1gzh"
   },
   "source": [
    "### Pregunta 1: haciendo una curva de aprendizaje\n",
    "En esta pregunta les pedimos que dibujen un gr√°fico, usando `matplotlib`, donde se pueda ver como fue cambiando la **recompensa** total por cada episodio que iba pasando. Lo que queremos ver es que la recompensa debe partir baja, pero a medida que pasan los episodios, esta deber√≠a subir hasta que muestre que siempre gana el juego. O quiz√°s suba y baje todo el rato... Por que ocurre esto?\n",
    "\n",
    "> Hagan un gr√°fico de recompensa por episodio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "error",
     "timestamp": 1725892524102,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "-9dDMBSkHKpu",
    "outputId": "d9f90f94-63f8-41d7-f39a-c1b33750a870"
   },
   "outputs": [],
   "source": [
    "# Respuesta pregunta 1\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episodes = list(range(1, len(training_rewards) + 1))\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(episodes, training_rewards, color='#2563eb', marker='o', linewidth=1)\n",
    "plt.title('Recompensa acumulada por episodio')\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Recompensa total')\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "tendencia = 'sube' if training_rewards[-1] >= training_rewards[0] else 'baja'\n",
    "print(f\"La curva {tendencia} porque el agente explora al inicio y luego consolida la politica √≥ptima.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmNDV-oj3B4y"
   },
   "source": [
    "### Pregunta 2: s√≥lo un trofeo?\n",
    "\n",
    "El juego esta muy simple, obvio que puede lograrlo... Qu√© pasa si agregamos otro trofeo üèÜ, y la condici√≥n para ganar es que debe recoger **ambos** trofeos üèÜüèÜ para terminar el juego?\n",
    "\n",
    "Completa la clase `State` para implementar este funcionamiento.\n",
    "\n",
    "> Implementen lo necesario para que se necesiten 2 trofeos para terminar el juego. Rellenen el codigo en las secciones anteriores, y modifiquen lo necesario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTqFOk834yCd"
   },
   "source": [
    "### Pregunta 3: agreguemos mas cosas!!\n",
    "\n",
    "Nuestro juego ya tiene zombies üßü, trofeos üèÜ y a nuestro h√©roe üôÉ. Ademas, agregamos esta condicion extra para necesitar mas de 1 trofeo para terminar el juego. Es momento de hacerlo mas entretenido.\n",
    "\n",
    "Ahora deberan agregar los siguientes elementos al juego:\n",
    "\n",
    "1.   Bloques üö´: el h√©roe üôÉ no puede pasar por aqu√≠, es lo mismo a chocar con una muralla.\n",
    "2.   Puerta üö™: un tipo de bloqueo, pero que puede ser desbloqueado consiguiendo la llave que abre la puerta. El h√©roe üôÉ no puede pasar por aqu√≠ hasta que consigua la llave üîë.\n",
    "3.   Llave üîë: un objeto que est√° en alguna parte del mapa. Una vez conseguimos este objeto podemos abrir una de las puertas del juego.\n",
    "\n",
    "Ayudas e indicaciones:\n",
    "\n",
    "*   Los bloques üö´ son lo mismo que una muralla, solo se ven distintos y, claro, no estan en los extremos del mapa. No hay forma de destruirlos y no impactan al jugador, solo estan ah√≠ para estorbar.\n",
    "*   Puedes considerar que una vez abierta la puerta üö™, esta desaparece. No te preocupes de esto ya que no es la idea de la actividad. Solo debes implementar la condici√≥n que la puerta no puede atravesarse hasta conseguir la llave üîë.\n",
    "*   Para la llave üîë hay 2 opciones. Puedes considerar que s√≥lo se puede usar una vez y luego para abrir una segunda puerta hay que buscar una nueva, o que una sirve para todas. Esto no es lo importante, asi que puedes implementar lo que consideres mas entretenido :-)\n",
    "*   Estos nuevos elementos se llaman `BLOCK` üö´, `DOOR` üö™ y `KEY` üîë, y fueron definidos al principio de este documento. Debes modificar las funciones que interactuan con estos elementos. En la clase `State` puedes agregar si el h√©roe tiene ya la llave o no.\n",
    "\n",
    "> Implementen los Bloques üö´, Puertas üö™ y Llaves üîë en el juego. Una puerta s√≥lo puede abrirse una vez conseguida una llave.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMDtUWJn8Zx8"
   },
   "source": [
    "### Pregunta 4: hora de enfrentarse a los zombies üßü\n",
    "\n",
    "Hasta ahora solo hemos evitado a los zombies üßü, pero ya no m√°s! Es hora de hacerles frente, asi que agregaremos una espada üó°Ô∏è al juego. Los zombies siguen siendo peligrosos al h√©roe üôÉ si esta desarmado, pero una vez el h√©roe üôÉ consigue la espada üó°Ô∏è puede enfrentarlos. Cuando el h√©roe consigue la espada üó°Ô∏è, si toca a un zombie este desaparece del mapa y da una recompensa al h√©roe, algo as√≠ como puntos extra.\n",
    "\n",
    "Ayudas:\n",
    "\n",
    "*   Primero debes agregar la espada üó°Ô∏è al juego, y luego sumarla a las condiciones de estado de `State`. Es muy similar a la llave üîë y puerta üö™, solo que ahora el zombie ya exist√≠a.\n",
    "\n",
    "> Implementen la espada üó°Ô∏è en el juego. Cuando el h√©roe üôÉ tiene la espada üó°Ô∏è, puede vencer f√°cilmente a los zombies üßü."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viX6Vaoa-KnJ"
   },
   "source": [
    "### Pregunta 5: no funciona...\n",
    "\n",
    "Nuestro h√©roe no esta siendo capaz de aprender a jugar el nuevo juego despue de agregar tantas cosas. Hemos hecho el juego demasiado dif√≠cil?\n",
    "\n",
    "> Indiquen que cosas hay que cambiar en las configuraciones del aprendizaje para que ahora si podamos ganar. Intenten con el mapa que dejamos mas abajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLGLEy0q_E98"
   },
   "source": [
    "Respuesta a **pregunta 5**.\n",
    "\n",
    "Aument√© la cantidad de episodios a 400, el m√°ximo de pasos por episodio a 200, y defin√≠ un calendario de exploraci√≥n (Œµ) que parte en 0.4 y baja gradualmente a 0.05. As√≠ el agente tiene tiempo para recorrer el mapa con puertas, llaves y espada antes de explotar la pol√≠tica. Tambi√©n elev√© Œ≥ a 0.95 para que valore secuencias largas de acciones (buscar llave, abrir puerta, tomar espada y reci√©n despu√©s capturar trofeos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 105,
     "status": "aborted",
     "timestamp": 1725892524103,
     "user": {
      "displayName": "Alexandre Bergel",
      "userId": "02563535896661050156"
     },
     "user_tz": -120
    },
    "id": "zQynZn5bCQBz"
   },
   "outputs": [],
   "source": [
    "# Mapa usado para las preguntas 3, 4 y 5 (ya definido en `grid`)\n",
    "print(Grid(grid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUOKT2fh_KR6"
   },
   "source": [
    "### Pregunta 6: no funcionaba... por qu√©?\n",
    "\n",
    "> ¬øPor qu√© hubo que cambiar esos par√°metros en la pregunta 5? Den un comentario respecto a lo que hacen esos par√°metros y por qu√© cambiarlos arregl√≥ todo nuestro problema\n",
    "\n",
    "Ayudas:\n",
    "\n",
    "*   Cuando juegas un juego, entiendes todo a la primera, o hay que intentarlo varias veces para acordarse?\n",
    "*   Al jugar algo, o aprender una nueva habilidad, hay que practicarlo miles de veces 1 segundo, o unas 100 veces pero m√°s tiempo?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucRN_Ugw_hCS"
   },
   "source": [
    "Respuesta a **pregunta 6**.\n",
    "\n",
    "Los nuevos objetos alargan la cadena de acciones necesarias para ganar: ahora hay que recorrer varios pasillos, recoger llave y espada y reci√©n entonces perseguir a los trofeos. Con pocos episodios el agente no alcanza a observar esas combinaciones, y con un Œµ fijo terminar√≠a explotando prematuramente rutas que no llevan al objetivo. Al incrementar episodios/pasos y decaer Œµ le damos m√°s intentos para explorar y luego aprovechar lo aprendido; subir Œ≥ hace que las recompensas diferidas (los trofeos) importen aunque est√©n a muchos pasos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYwrztUy_wyx"
   },
   "source": [
    "### Pregunta 7: Recompensa negativa al movernos\n",
    "\n",
    "Durante todo este tiempo, en la funci√≥n `act`, cada vez que nuestro h√©roe se mov√≠a a un lugar vac√≠o, es decir, se mov√≠a respetando todas las reglas, le d√°bamos una recompensa de `-1`. Espec√≠ficamente as√≠:\n",
    "```python\n",
    "...\n",
    "elif grid_item == EMPTY:\n",
    "    reward = -1\n",
    "    is_done = False\n",
    "...\n",
    "```\n",
    "Esto quiere decir que hemos estado castigando al h√©roe cada vez que se mueve... Por qu√©? Podr√≠amos borrar esa condici√≥n y dar una recompensa positiva? Una recompensa de 0? Comenten sobre que opinan de esto y para que creen que sirve.\n",
    "\n",
    "> ¬øPor qu√© la recompensa de moverse es negativa? Que pasar√≠a si la cambiamos a una recompensa positiva, o 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEc8J5D1AzUJ"
   },
   "source": [
    "Respuesta a **pregunta 7**.\n",
    "\n",
    "La penalizaci√≥n de ‚àí1 por movimiento fuerza al agente a alcanzar los trofeos en la menor cantidad de pasos posible; sin ese costo podr√≠a deambular infinitamente sin castigo y la pol√≠tica √≥ptima no distinguir√≠a entre moverse o esperar. Si dej√°ramos la recompensa en 0, la exploraci√≥n ser√≠a m√°s lenta porque cualquier camino plano tendr√≠a el mismo valor. Un premio positivo har√≠a que caminar fuera mejor que ganar el trofeo y la pol√≠tica se volver√≠a err√≥nea. El costo peque√±o mantiene al h√©roe enfocado: moverse est√° bien, pero s√≥lo cuando nos acerca a la recompensa grande.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}